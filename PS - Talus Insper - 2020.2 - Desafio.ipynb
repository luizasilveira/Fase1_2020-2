{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESSO SELETIVO TALUS INSPER 2020.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olá! Esse é o Jupyter com o desafio do processo seletivo para a Talus!\n",
    "\n",
    "É aqui que deve ser posta a resolução do desafio que iremos propor e nenhum outro material entregue junto com este será considerado. Antes de continuarmos para o desafio, precisamos que você se identifique (usuários não identificados podem até passar, mas não receberão notificação &#128521;\n",
    "\n",
    "Edite essa célula e\n",
    "\n",
    "<font color='red'>Luiza Rodrigues Silveira</font>\n",
    "\n",
    "<font color='red'>luizars2@al.insper.edu.br</font>\n",
    "\n",
    "Se você tiver um usuário do Discord\n",
    "\n",
    "<font color='red'>COLOQUE SEU USUÁRIO AQUI</font>\n",
    "\n",
    "Lembre que não é obrigatório Discord nessa etapa, mas ambas Segunda e Terceira Fase serão realizadas por lá. Nós estamos num servidor do Discord especial feito pra vocês, você pode passar lá e tirar dúvidas com nossos membros a qualquer momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regras do desafio\n",
    "\n",
    "O desafio que propomos aqui é construir um modelo de regressão linear simples para apenas uma variável.\n",
    "\n",
    "Existem diversas maneiras de fazer isso, a maneira que iremos explicar aqui, e que você deverá reproduzir, é o método de Gradient Descent. Qualquer outro método que não este __NÃO__ será considerado.\n",
    "\n",
    "Uma análise exploratória dos dados não é obrigatória e nem mesmo necessária!\n",
    "\n",
    "Por último, vale frisar: a utilização de pacotes com funções que cortem passos ou que façam o trabalho por você resultará na nulidade de sua solução e __NÃO__ será considerado também.\n",
    "\n",
    "No entanto, você pode usar os pacotes que foram ensinados no arquivo de tutorial para esse desafio.\n",
    "\n",
    "Você irá achar algumas células com código, elas servem para guiar você, mas não são obrigatórias! E você não precisa usar a estrutura que propomos, mas deve seguir o roteiro que se encontra no fim do desafio!\n",
    "\n",
    "Boa sorte, nós estamos esperando por você na Talus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposta de desafio\n",
    "\n",
    "Como dito, o seu desafio será transformar em código a teoria sobre modelos de regressão linear que será ensinada aqui.\n",
    "\n",
    "Você usará o código que criou para prever o comportamento de uma variável em função de outra, estas podem ser achadas no dataset 'desafio.csv' na pasta data. As variáveis foram geradas manualmente por nós e portanto asseguramos que existe uma relação linear entre elas.\n",
    "\n",
    "No nosso dataset, a variável que será prevista é a variável y. Não existe um valor a ser batido, mas existem com certeza valores visivelmente incorretos. Seu código não será avaliado apenas pelo valor dos coeficientes, mas também (e principalmente) pela qualidade do código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicação do modelo\n",
    "\n",
    "Sem entrarmos nos detalhes matemáticos (você irá aprender isso conosco depois), uma regressão linear é um modelo capaz de computar o valor de uma variável através de uma soma com pesos de outras variáveis mais a adição de uma constante (também chamada de viés ou intercepto). De maneira geral, uma relação linear entre variáveis pode ser expressa por:\n",
    "\n",
    "$$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... \\theta_nx_n$$\n",
    "\n",
    "Aqui trataremos apenas do caso $n = 1$, sendo $n$ o número de features ou inputs do nosso modelo.\n",
    "\n",
    "Nessa equação, $\\theta_i$ é o parâmetro da feature $i$ sendo $\\theta_0$ o viés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leia nessa célula o dataset \"desafio.csv\" e obtenha os arrays de X e y\n",
    "\n",
    "dataset = pd.read_csv(\"data/desafio.csv\")\n",
    "\n",
    "X = dataset[\"X\"]\n",
    "y = dataset[\"y\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os algoritmos de regressão linear servem para acharmos, de maneira mais eficiente, os parâmetros $\\theta$ de nosso modelo, definida uma métrica.\n",
    "\n",
    "Portanto, antes de falarmos como construirmos e treinarmos um modelo desses, é preciso definir nossa métrica.\n",
    "\n",
    "Existem diversas métricas de avaliação quando falamos de modelos lineares, a mais popular e que usaremos aqui é o __Erro Quadrático Médio__ ou (EQM) que é dado por:\n",
    "\n",
    "$$EQM(ŷ) = \\frac{1}{m}\\sum^m_{i=1}(ŷ_i - y_i)²$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "$m$ é o número de amostras usada no modelo;\n",
    "\n",
    "$ŷ$ é o valor previsto por nosso modelo;\n",
    "\n",
    "$y$ é o valor real da variável prevista.\n",
    "\n",
    "Substituindo a equação linear na fórmula do EQM ficamos com:\n",
    "\n",
    "$$EQM(ŷ) = \\frac{1}{m}\\sum^m_{i=1}(\\theta_1{x_1}_i + \\theta_0 - y_i)²$$\n",
    "\n",
    "E, portanto, vemos que $EQM$ depende do termo quadrático de $\\theta_1$ e $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crie aqui uma função que calcule EQM\n",
    "# antes, lembre de inicializar o parâmetro m do seu modelo\n",
    "\n",
    "m = dataset.shape[0]\n",
    "\n",
    "# os parâmetros recebidos pela função ficam a seu critério\n",
    "\n",
    "def calcula_eqm(m, theta0, theta1):\n",
    "    dataset = pd.read_csv(\"data/desafio.csv\")\n",
    "    X = dataset[\"X\"]\n",
    "    y = dataset[\"y\"]\n",
    "    soma = 0   \n",
    "    for i in range(0,m):\n",
    "        soma += (theta1*X[i] + theta0 - y[i])**2        \n",
    "        eqm = (1/m) * soma\n",
    "        \n",
    "    return eqm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dito, existem várias maneiras de encontrar os parâmetros do nosso modelo, inclusive, um método bem mais simples do que o que vamos ensinar (mas que é BEM mais lento para uma quantidade grande de dados).\n",
    "\n",
    "O método que usaremos se chama *Gradient Descent*, ele é um algoritmo, ou melhor, uma família de algoritmos, bem simples e genérico capaz de encontrar os parâmetros de nossa regressão de uma maneira mais rápida, objetivando minimizar o valor do nosso erro, o $EQM$.\n",
    "\n",
    "Esse método consiste de inicializar, aleatoriamente, o valor dos parâmetros e, iterativamente, modificar esse valor em função do erro obtido.\n",
    "\n",
    "Para explicar bem o procedimento, vamos supor $\\theta_0$ (ou $\\theta_1$) constante. Nesse caso, teríamos que $EQM$ é uma função quadrática de $\\theta_1$, ou seja, uma parábola.\n",
    "\n",
    "Nesse caso, o gráfico de $EQM$ x $\\theta_1$ seria semelhante a:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/600/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg\" height=\"400\" width=\"600\">\n",
    "\n",
    "Onde cost é a função de custo, que no caso é $EQM$.\n",
    "\n",
    "O que queremos é dar, pequenos \"passos\", modificando $\\theta_1$ até alcançarmos aquele que minimiza nosso erro.\n",
    "\n",
    "Caso você não tenha entedido ainda, façamos um exercício de imaginação: imagine que você esteja preso no topo de uma montanha durante uma névoa muito densa, deixando de lado suas habilidades de alpinismo, uma maneira de achar a base da montanha seria deslizar seu pé no chão até achar a direção de descida e então dar pequenos passos nessa direção, é exatamente isso que vamos fazer aqui.\n",
    "\n",
    "O learning step no nosso gráfico seria o tamanho do passo que você daria na montanha e a direção que seu pé indicaria seria o quê? Vamos ver isso agora.\n",
    "\n",
    "Ah, e claro você esteja se coçando que não resolvemos o caso real, com $n$ features, calma, não é o foco desse desafio e não é muito diferente da ideia que mostramos aqui, você só teria que ter uma abstração maior para desenhar o gráfico.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1098/1*yasmQ5kvlmbYMe8eDkyl6w.png\" height=\"400\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tá certo, mas antes de falar sobre a direção do passo, é importante falarmos algo sobre o learning step, ou learning rate como iremos chamar agora.\n",
    "\n",
    "Você se perguntou qual o valor que o learning rate deveria ter? Bom, nós não vamos te dar uma resposta sobre isso, mas vamos te mostrar o que o valor que você escolheu poderia resultar.\n",
    "\n",
    "Se você escolher um learning rate muito pequeno, o seu modelo precisaria de muito mais iterações e execuções para achar o valor de mínimo. É o equivalente a você dar passos que mal separam suas pernas tentando descer a montanha.\n",
    "\n",
    "Já um learning rate muito grande corre o risco de passar do local de mínimo.\n",
    "\n",
    "Veja essas imagens que exemplificam bem isso, mostrando um caso com learning rate pequeno e outro com learning rate muito grande, respectivamente:\n",
    "\n",
    "<div style=\"display: block\">\n",
    "<img src=\"./img/small_lr.png\" height=\"400\" width=\"600\">\n",
    "\n",
    "<img src=\"./img/large_lr.png\" height=\"400\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "O learning rate também é importante para se esquivar de mínimos locais, mas você não precisa se preocupar com isso aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialize aqui os parâmetros learning rate e o theta_0 e theta_1 inicial\n",
    "\n",
    "theta_0_ini = 0\n",
    "theta_1_ini = 0\n",
    "\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, agora vamos pro último fundamento (e o mais importante) pra que você possa concluir o desafio.\n",
    "\n",
    "A análogo matemático da direção do seu passo tentando descer da montanha é o que dá nome a esse algoritmo, o *Gradiente*.\n",
    "\n",
    "Imaginamos que você esteja familiarizado com o conceito de derivada. Imagine no primeiro gráfico que lhe apresentamos que você tivesse que apontar a direção para onde o valor de $\\theta$ precisa andar, talvez você tenha feito com o dedo uma linha tangente apontando para o próximo ponto do gráfico.\n",
    "\n",
    "Devemos achar a tangente, ou mais especificamente, o coeficiente angular desta, para encontrarmos a direção que devemos seguir a fim de minimizar nossa função.\n",
    "\n",
    "E, uma luz pode ter acendido em você, indicando o que será necessário para tal feito: as derivadas.\n",
    "\n",
    "Se calcularmos a derivada para $\\theta_0$ e $\\theta_1$ teríamos:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_0}EQM = \\frac{2}{m}\\sum^m_{i=1}(\\theta_1{x_1}_i + \\theta_0 - y_i)$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_1}EQM = \\frac{2}{m}\\sum^m_{i=1}(\\theta_1{x_1}_i + \\theta_0 - y_i){x_1}_i$$\n",
    "\n",
    "O símbolo pode parecer diferente, mas a ideia é a mesma, é que nesse caso estamos falando de derivada parcial.\n",
    "\n",
    "Ah, e o motivo do nome gradiente, vem porque um gradiente basicamente é um vetor formado pela derivada parcial das variáveis de que depende uma função e indico sentido e a direção cujo deslocamento maximiza ou minimiza um valor especificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crie aqui as funções que calculam as derivadas parciais de theta_0 e theta_1\n",
    "# não esqueça os parâmetros\n",
    "\n",
    "def dif_theta_0(m,theta0, theta1):\n",
    "    dataset = pd.read_csv(\"data/desafio.csv\")\n",
    "    X = dataset[\"X\"]\n",
    "    y = dataset[\"y\"]\n",
    "    soma = 0   \n",
    "    for i in range(0,m-1, 1):\n",
    "        soma += (theta1*X[i] + theta0 - y[i])\n",
    "        \n",
    "    grad = soma * (2/m)\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def dif_theta_1(m,theta0, theta1):\n",
    "    dataset = pd.read_csv(\"data/desafio.csv\")\n",
    "    X = dataset[\"X\"]\n",
    "    y = dataset[\"y\"]\n",
    "    soma = 0\n",
    "    for i in range(0,m-1, 1):\n",
    "        soma += (theta1*X[i] + theta0 - y[i])*X[i]\n",
    "    \n",
    "        \n",
    "    grad = soma * (2/m)\n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, agora você tem tudo que precisa para montar o modelo, juntando todas as ideias obtidas até aqui, o processo por meio do qual você irá iterar os parâmetros da sua regressão é o seguinte:\n",
    "\n",
    "$$\\theta_{i+1} = \\theta_{i} - \\mu\\frac{\\partial}{\\partial\\theta}EQM$$\n",
    "\n",
    "Onde\n",
    "\n",
    "$\\theta_i$ é o valor de $\\theta$ (0 ou 1) na i-ésima iteração;\n",
    "\n",
    "$\\mu$ é o learning rate.\n",
    "\n",
    "Com isso, você pode achar o parâmetros da regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crie aqui a função que realiza a iteração de theta\n",
    "\n",
    "def itera_theta(m,theta0_ini, theta1_ini, learning_rate):\n",
    "    theta_0 = theta0_ini\n",
    "    theta_1 = theta1_ini\n",
    "    for e in range(0,m-1,1) :\n",
    "        next_theta0 = theta_0 - (learning_rate * dif_theta_0(m,theta_0, theta_1))\n",
    "        theta_0 = next_theta0\n",
    "        next_theta1 = theta_1 - (learning_rate * dif_theta_1(m,theta_0, theta_1))\n",
    "        theta_1 = next_theta1\n",
    "\n",
    "    return theta_0, theta_1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto, a partir daqui é com você. Mas nós vamos dar mais uma mãozinha. Nós preparamos um roteiro e __ATENÇÃO__ todos os pontos são __OBRIGATÓRIOS__ mas a execução é totalmente por sua conta.\n",
    "\n",
    "- Leia o arquivo \"desafio.csv\" na pasta data\n",
    "- Obtenha as variáveis X e y no dataset\n",
    "- Inicialize (e deixe bem claro onde fez isso) os parâmetros de seu modelo:\n",
    "    - Learning rate;\n",
    "    - Número de iterações;\n",
    "    - Número de amostras;\n",
    "    - $\\theta_0$ e $\\theta_1$ iniciais, gerados aleatoriamente.\n",
    "- Desenvolva a função que calcula EQM\n",
    "- Desenvolva uma (ou duas) funções que devolva a derivada parcial de EQM para cada um dos $\\theta$\n",
    "- Desenvolva a função que itera $\\theta_0$ e $\\theta_1$ e devolva os valores finais\n",
    "- Plote um gráfico contendo:\n",
    "    - Os valores reais de X e y\n",
    "    - A reta formada pelos valores de $\\theta$ encontrados por você\n",
    "    \n",
    "Ainda que você não consiga concretizar um dos passos, NÃO desista. Novamente, o foco não é no resultado, mas na qualidade de seu código.\n",
    "\n",
    "__BOA SORTE!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os parâmetros recebidos pela função ficam a seu critério\n",
    "\n",
    "#def calcula_eqm(m, theta0, theta1):\n",
    "#def dif_theta_0(m,theta0, theta1):\n",
    "#def itera_theta(m,theta0_ini, theta1_ini, learning_rate):\n",
    "\n",
    "t0 = itera_theta(m,theta_0_ini, theta_1_ini, learning_rate)[0]\n",
    "t1 = itera_theta(m,theta_0_ini, theta_1_ini, learning_rate)[1]\n",
    "eqm = calcula_eqm(m, t0, t1)\n",
    "\n",
    "#𝑦=𝜃0+𝜃1𝑥1+𝜃2𝑥2+...𝜃𝑛𝑥𝑛\n",
    "novo_y = []\n",
    "for e in range(0,m):\n",
    "    novo_y.append(t0 + X[e]*t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(dataset['X'],dataset['y'])\n",
    "plt.plot(X,novo_y, color = \"green\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
